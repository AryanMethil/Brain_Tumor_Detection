{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Melody_Generation_Preprocesing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanMethil/Brain_Tumor_Detection/blob/master/Melody_Generation_Preprocesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keIibv4Et9qA"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/deutschl.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNDH5ti2GgQz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb9af91b-1781-4883-c050-795b64512b90"
      },
      "source": [
        "!pip install music21"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjA8X1LMNIi_"
      },
      "source": [
        "import music21 as m21"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEfhWvd7MEtY"
      },
      "source": [
        "KERN_DATASET_PATH='/content/essen/europa/deutschl/erk'\n",
        "ACCEPTABLE_DURATIONS=[0.25,0.5,0.75,1,1.5,2,3,4]\n",
        "SINGLE_FILE_DATASET='file_dataset'\n",
        "SEQUENCE_LENGTH=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gtJZcHCMsZD"
      },
      "source": [
        "def load_songs_in_kern(dataset_path):\n",
        "\n",
        "  songs=[]\n",
        "\n",
        "  #go through the files and load them using music21\n",
        "  for dir,subdir,files in os.walk(dataset_path):\n",
        "    for f in files:\n",
        "      if(f[-3:]=='krn'):\n",
        "        song=m21.converter.parse(os.path.join(dir,f))\n",
        "        songs.append(song)\n",
        "  return songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwAxjjxLxFYh"
      },
      "source": [
        "def has_acceptable_durations(song,ACCEPTABLE_DURATIONS):\n",
        "  for note in song.flat.notesAndRests:\n",
        "    if(note.duration.quarterLength not in ACCEPTABLE_DURATIONS):\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKzOep6y7eM"
      },
      "source": [
        "def transpose(song):\n",
        "\n",
        "  #get key from the song\n",
        "  parts=song.getElementsByClass(m21.stream.Part)\n",
        "  measures_part0=parts[0].getElementsByClass(m21.stream.Measure)\n",
        "  key=measures_part0[0][4]\n",
        "\n",
        "  #estimate key using music21\n",
        "  if not isinstance(key,m21.key.Key):\n",
        "    key=song.analyze('key')\n",
        "\n",
        "  #get interval for transposition\n",
        "  if key.mode=='major':\n",
        "    interval=m21.interval.Interval(key.tonic,m21.pitch.Pitch('C'))\n",
        "  elif key.mode=='minor':\n",
        "    interval=m21.interval.Interval(key.tonic,m21.pitch.Pitch('A'))\n",
        "    \n",
        "  #transpose song by calculated interval\n",
        "  transposed_song=song.transpose(interval)\n",
        "\n",
        "  return transposed_song"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZFaSRr5Hcz"
      },
      "source": [
        "def encode_song(song):\n",
        "\n",
        "  encoded_song=[]\n",
        "  \n",
        "  # pitch=60 duration=1 -> [60,'_','_','_']\n",
        "  # ie each element in the encoded list will represent a quarter length \n",
        "  # so pitch 60 will be there for 4 elements but we write only the first occurence as 60 and the others as underscore\n",
        "\n",
        "  for event in song.flat.notesAndRests:\n",
        "    \n",
        "    # handle notes\n",
        "    if isinstance(event,m21.note.Note):\n",
        "      symbol=event.pitch.midi #60\n",
        "    \n",
        "    elif isinstance(event,m21.note.Rest):\n",
        "      symbol='r'\n",
        "    \n",
        "    steps=int(event.duration.quarterLength/0.25)\n",
        "\n",
        "    for step in range(steps):\n",
        "      if step==0:\n",
        "        encoded_song.append(symbol)\n",
        "      else:\n",
        "        encoded_song.append('_')\n",
        "\n",
        "  encoded_song=\" \".join(list(map(str,encoded_song)))\n",
        "\n",
        "  return encoded_song"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z01DSSNX6C8I"
      },
      "source": [
        "os.makedirs('/content/dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LjASayML8BT"
      },
      "source": [
        "def preprocess(dataset_path):\n",
        "\n",
        "  #load the filter songs\n",
        "  songs=load_songs_in_kern(dataset_path)\n",
        "\n",
        "  for i,song in enumerate(songs):\n",
        "\n",
        "    #filter out songs that have non-acceptable durations\n",
        "    if not has_acceptable_durations(song,ACCEPTABLE_DURATIONS):\n",
        "      continue\n",
        "\n",
        "    #transpose songs to C major / A minor\n",
        "    song=transpose(song)\n",
        "\n",
        "\n",
        "    #encode songs with music time series representation\n",
        "    encoded_song=encode_song(song)\n",
        "\n",
        "    #save songs to text file\n",
        "    save_path='/content/dataset/'+str(i)\n",
        "    with open(save_path,'w') as f:\n",
        "      f.write(encoded_song)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9C5Y_DvaVhW"
      },
      "source": [
        "preprocess(KERN_DATASET_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqS-BEbe9hoE"
      },
      "source": [
        "def load(file_path):\n",
        "  with open(file_path,'r') as f:\n",
        "    song=f.read()\n",
        "  return song"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZzgpRg78oYC"
      },
      "source": [
        "def create_single_file_dataset(dataset_path,single_file_dataset,sequence_length):\n",
        "\n",
        "  new_song_delimiter=\"/ \"*sequence_length\n",
        "  songs=\"\"\n",
        "  # load encoded songs and add delimiters\n",
        "  for dir,subdir,files in os.walk(dataset_path):\n",
        "    for f in files:\n",
        "      file_path=os.path.join(dir,f)\n",
        "      song=load(file_path)\n",
        "      songs+=song+\" \"+new_song_delimiter\n",
        "  songs=songs[:-1]\n",
        "  # save string that contains all dataset\n",
        "  with open(single_file_dataset,'w') as f:\n",
        "    f.write(songs)\n",
        "  \n",
        "  return songs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PQdgnmm-kh1"
      },
      "source": [
        "songs=create_single_file_dataset('/content/dataset/',SINGLE_FILE_DATASET,SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SeVI1i6-8pM"
      },
      "source": [
        "import json\n",
        "def create_mapping(songs):\n",
        "  \n",
        "  mapping={}\n",
        "\n",
        "  # identify the vocabulary\n",
        "  songs=songs.split()\n",
        "  vocabulary=list(set(songs))\n",
        "\n",
        "  # create mapping\n",
        "  for i,symbol in enumerate(vocabulary):\n",
        "    mapping[symbol]=i\n",
        "\n",
        "  # save the vocabulary in json file\n",
        "  with open('mapping.json','w') as f:\n",
        "    json.dump(mapping,f,indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u40UY7E5Gn5M"
      },
      "source": [
        "create_mapping(songs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-NfE_yEG90c"
      },
      "source": [
        "def convert_songs_to_int(songs):\n",
        "\n",
        "  int_songs=[]\n",
        "\n",
        "  # load mappings\n",
        "  with open('mapping.json','r') as f:\n",
        "    mapping=json.load(f)\n",
        "\n",
        "  # songs to list\n",
        "  songs=songs.split()\n",
        "\n",
        "  # map strings to int\n",
        "  for symbol in songs:\n",
        "    int_songs.append(mapping[symbol])\n",
        "  \n",
        "  return int_songs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuo1g-Z7JUpo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def generate_training_sequences(sequence_length):\n",
        "\n",
        "  # load the songs and map them to int\n",
        "  songs=load(SINGLE_FILE_DATASET)\n",
        "  int_songs=convert_songs_to_int(songs)\n",
        "\n",
        "  # generate the training sequences\n",
        "  inputs=[]\n",
        "  targets=[]\n",
        "\n",
        "  num_sequences=len(int_songs)-sequence_length\n",
        "  for i in range(num_sequences):\n",
        "    inputs.append(int_songs[i:i+sequence_length])\n",
        "    targets.append(int_songs[i+sequence_length])\n",
        "  \n",
        "  # onehot encode the sequences\n",
        "  # inputs shape = num_sequences x sequence_length\n",
        "  # [[0,1,2],[1,1,2]] -> [ [ [1,0,0],[0,1,0],[0,0,1] ] , [ [],[],[] ] ]\n",
        "  # one-hot encoded shape -> num_sequences x sequence_length x vocabulary_size\n",
        "\n",
        "  vocabulary_size=len(set(int_songs))\n",
        "  inputs=tf.keras.utils.to_categorical(inputs,num_classes=vocabulary_size)\n",
        "  targets=np.array(targets)\n",
        "  return inputs,targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrms0L5U9RRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c1433e3-410a-4b6f-a068-91b391ba0e69"
      },
      "source": [
        "inputs,targets=generate_training_sequences(SEQUENCE_LENGTH)\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(362178, 64, 38)\n",
            "(362178,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXM5eFTz96uc"
      },
      "source": [
        "from shutil import make_archive,rmtree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPim47c4G5sw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69d091fa-6765-4f04-f6dc-7d92987adcad"
      },
      "source": [
        "make_archive('dataset','zip','/content/dataset/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/dataset.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZTSzeHzZK6t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}